{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d811efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "171519ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDataset(Dataset):\n",
    "    def __init__(self,audio_dir,transformation,target_sample_rate,num_samples,device):\n",
    "        self.df = pd.read_csv('UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def __get_path(self,index):\n",
    "        sample = self.df.iloc[index]\n",
    "        path = self.audio_dir + f'fold{sample['fold']}/' + sample['slice_file_name'] \n",
    "        print ( path)\n",
    "        return path\n",
    "\n",
    "    def __get_label(self,index):\n",
    "        sample = self.df.iloc[index]\n",
    "        return sample['classID']\n",
    "    \n",
    "\n",
    "    def __resample(self,signal,sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr , self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "\n",
    "        return signal\n",
    "    \n",
    "    def __mix_down(self,signal):\n",
    "        if signal.dim() > 1 and signal.size(0) > 1: # (2,1000) , if it isn't mono\n",
    "            signal = torch.mean(signal, dim = 0 ,keepdim = True)\n",
    "        \n",
    "        return signal\n",
    "\n",
    "    def __cut(self,signal):\n",
    "        if signal.shape[1] > self.num_samples :\n",
    "            signal[:, :self.num_samples]\n",
    "\n",
    "        return signal\n",
    "\n",
    "    def __right_pad(self,signal):\n",
    "        signal_lenght = signal.shape[1]\n",
    "        if signal_lenght < self.num_samples:\n",
    "            missing_samples_num = self.num_samples - signal_lenght\n",
    "            padding = (0,missing_samples_num)\n",
    "            signal = torch.nn.functional.pad(signal,padding)\n",
    "\n",
    "        return signal  \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self.__get_path(index)\n",
    "        label = self.__get_label(index)\n",
    "        signal , sr = torchaudio.load(audio_sample_path,format=\"wav\")\n",
    "        signal = signal.to(self.device)\n",
    "        print('Signal : ', len(signal))\n",
    "        print('Sample rate : ',sr)\n",
    "        # signal -> (num_channels , samples) -> (2 , 16000) \n",
    "        signal = self.__resample(signal,sr)\n",
    "        signal = self.__mix_down(signal)\n",
    "        signal = self.__cut(signal)\n",
    "        signal = self.__right_pad(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        print('Signal : ', len(signal))\n",
    "        return signal , label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6167d276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "AUDIO_DIR = '/home/furkan/AudioDeepLearning/UrbanSound8K/audio/'\n",
    "SAMPLE_RATE = 22050\n",
    "NUM_SAMPLES = 22050\n",
    "\n",
    "transform = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate = SAMPLE_RATE,\n",
    "    n_fft = 1024,\n",
    "    hop_length = 512,\n",
    "    n_mels = 64\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device\",device)\n",
    "\n",
    "\n",
    "dataset = SoundDataset(AUDIO_DIR,transform,SAMPLE_RATE,NUM_SAMPLES,device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46a0e60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "212cc18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/furkan/AudioDeepLearning/UrbanSound8K/audio/fold5/100032-3-0-0.wav\n",
      "Signal :  2\n",
      "Sample rate :  44100\n",
      "Signal :  1\n"
     ]
    }
   ],
   "source": [
    "signal , label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb185bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundNeauralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 4 conv , flatten , linear , softmax\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear = nn.Linear(128 * 5 * 4 , 10)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "\n",
    "    def forward(self,input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        predictions = self.softmax(logits)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a0db594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoundNeauralNetwork(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear): Linear(in_features=2560, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SoundNeauralNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040f74d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

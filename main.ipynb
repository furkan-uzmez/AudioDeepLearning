{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d811efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "171519ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDataset(Dataset):\n",
    "    def __init__(self,audio_dir,transformation,target_sample_rate,num_samples,device):\n",
    "        self.df = pd.read_csv('UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def __get_path(self,index):\n",
    "        sample = self.df.iloc[index]\n",
    "        path = os.path.join(self.audio_dir, f\"fold{sample['fold']}\", sample['slice_file_name'])\n",
    "        return path\n",
    "\n",
    "    def __get_label(self,index):\n",
    "        sample = self.df.iloc[index]\n",
    "        return sample['classID']\n",
    "    \n",
    "\n",
    "    def __resample(self,signal,sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            signal = signal.cpu()\n",
    "            resampler = torchaudio.transforms.Resample(sr , self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "\n",
    "        return signal\n",
    "    \n",
    "    def __mix_down(self,signal):\n",
    "        if signal.dim() > 1 and signal.size(0) > 1: # (2,1000) , if it isn't mono\n",
    "            signal = torch.mean(signal, dim = 0 ,keepdim = True)\n",
    "        \n",
    "        return signal\n",
    "\n",
    "    def __cut(self,signal):\n",
    "        if signal.shape[1] > self.num_samples :\n",
    "            signal = signal[:, :self.num_samples]\n",
    "\n",
    "        return signal\n",
    "\n",
    "    def __right_pad(self,signal):\n",
    "        signal_lenght = signal.shape[1]\n",
    "        if signal_lenght < self.num_samples:\n",
    "            missing_samples_num = self.num_samples - signal_lenght\n",
    "            padding = (0,missing_samples_num)\n",
    "            signal = torch.nn.functional.pad(signal,padding)\n",
    "\n",
    "        return signal  \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            audio_sample_path = self.__get_path(index)\n",
    "            label = self.__get_label(index)\n",
    "            signal , sr = torchaudio.load(audio_sample_path, format=\"wav\")\n",
    "            #signal = signal.to(self.device)\n",
    "\n",
    "            signal = self.__resample(signal, sr)\n",
    "            signal = self.__mix_down(signal)\n",
    "            signal = self.__cut(signal)\n",
    "            signal = self.__right_pad(signal)\n",
    "            signal = self.transformation(signal)\n",
    "\n",
    "            return signal, label\n",
    "        except Exception as e:\n",
    "            print(f\"Hata: {e} -> Index: {index}\")\n",
    "            return self.__getitem__((index + 1) % len(self.df))  # bir sonraki veriyle devam et\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6167d276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "AUDIO_DIR = '/home/furkan/AudioDeepLearning/UrbanSound8K/audio/'\n",
    "SAMPLE_RATE = 22050\n",
    "NUM_SAMPLES = 22050\n",
    "LEARNING_RATE = .001\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 10\n",
    "\n",
    "transform = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate = SAMPLE_RATE,\n",
    "    n_fft = 1024,\n",
    "    hop_length = 512,\n",
    "    n_mels = 64\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device\",device)\n",
    "\n",
    "\n",
    "dataset = SoundDataset(AUDIO_DIR,transform,SAMPLE_RATE,NUM_SAMPLES,device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46a0e60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "212cc18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal , label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb185bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundNeauralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 4 conv , flatten , linear , softmax\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear = nn.Linear(128 * 5 * 4 , 10)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "\n",
    "    def forward(self,input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        predictions = self.softmax(logits)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a0db594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoundNeauralNetwork(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear): Linear(in_features=2560, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SoundNeauralNetwork().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf059fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8726ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(),lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "040f74d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:14<00:00,  8.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.211721420288086\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:13<00:00,  8.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.2201600074768066\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:15<00:00,  8.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.25758695602417\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:14<00:00,  8.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.0946662425994873\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:11<00:00,  7.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.058579206466675\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:14<00:00,  8.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.0330209732055664\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:11<00:00,  7.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.0200397968292236\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:16<00:00,  8.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.06611967086792\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:06<00:00,  7.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 1.9976367950439453\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:15<00:00,  8.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.065526247024536\n",
      "model trained and stored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    print(f\"Epoch {i+1}\")\n",
    "    for inputs , targets in tqdm(data_loader):\n",
    "        inputs , targets = inputs.to(device) , targets.to(device)\n",
    "\n",
    "        predictions = model(inputs)\n",
    "\n",
    "        loss = loss_f(predictions,targets)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "    print(f\"Loss : {loss}\")\n",
    "\n",
    "torch.save(model.state_dict(),\"model.pth\")\n",
    "print(\"model trained and stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39b06913",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = [\n",
    "    \"air_conditioner\",\n",
    "    \"car_horn\",\n",
    "    \"children_playing\",\n",
    "    \"dog_bark\",\n",
    "    \"drilling\",\n",
    "    \"engine_idling\",\n",
    "    \"gun_shot\",\n",
    "    \"jackhammer\",\n",
    "    \"siren\", \n",
    "    \"street_music\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9eb168a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[8.1443e-04, 2.1588e-04, 9.1436e-04,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [2.4870e-03, 1.5724e-03, 4.2855e-04,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [2.9685e-03, 6.0449e-03, 4.1659e-03,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [1.7061e-04, 5.7996e-02, 7.9969e-01,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [2.1515e-04, 1.5781e-02, 3.7936e-01,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [3.0015e-04, 1.4416e-02, 3.1233e-01,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00]]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (batch_size , num_channels , fr ,time)\n",
    "input , target = dataset[0][0] , dataset[0][1]\n",
    "input.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e802d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted : dog_bark , Expected : dog_bark\n"
     ]
    }
   ],
   "source": [
    "model = model.to(\"cpu\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(input)\n",
    "    \n",
    "    prediction_index = predictions[0].argmax(0)\n",
    "    predicted = class_mapping[prediction_index]\n",
    "    \n",
    "expected = class_mapping[target]\n",
    "\n",
    "print(f\"Predicted : {predicted} , Expected : {expected}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

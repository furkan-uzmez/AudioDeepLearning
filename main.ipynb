{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d811efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171519ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDataset(Dataset):\n",
    "    def __init__(self,audio_dir,transformation,target_sample_rate,num_samples):\n",
    "        self.df = pd.read_csv('UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "        self.audio_dir = audio_dir\n",
    "        self.transformation = transformation\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def __get_path(self,index):\n",
    "        sample = self.df.iloc[index]\n",
    "        path = self.audio_dir + f'fold{sample['fold']}/' + sample['slice_file_name'] \n",
    "        print ( path)\n",
    "        return path\n",
    "\n",
    "    def __get_label(self,index):\n",
    "        sample = self.df.iloc[index]\n",
    "        return sample['classID']\n",
    "    \n",
    "\n",
    "    def __resample(self,signal,sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr , self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "\n",
    "        return signal\n",
    "    \n",
    "    def __mix_down(self,signal):\n",
    "        if signal.dim() > 1 and signal.size(0) > 1: # (2,1000) , if it isn't mono\n",
    "            signal = torch.mean(signal, dim = 0 ,keepdim = True)\n",
    "        \n",
    "        return signal\n",
    "\n",
    "    def __cut(self,signal):\n",
    "        if signal.shape[1] > self.num_samples :\n",
    "            signal[:, :self.num_samples]\n",
    "\n",
    "        return signal\n",
    "\n",
    "    def __right_pad(self,signal):\n",
    "        signal_lenght = signal.shape[1]\n",
    "        if signal_lenght < self.num_samples:\n",
    "            missing_samples_num = self.num_samples - signal_lenght\n",
    "            padding = (0,missing_samples_num)\n",
    "            signal = torch.nn.functional.pad(signal,padding)\n",
    "\n",
    "        return signal  \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self.__get_path(index)\n",
    "        label = self.__get_label(index)\n",
    "        signal , sr = torchaudio.load(audio_sample_path,format=\"wav\")\n",
    "        print('Signal : ', len(signal))\n",
    "        print('Sample rate : ',sr)\n",
    "        # signal -> (num_channels , samples) -> (2 , 16000) \n",
    "        signal = self.__resample(signal,sr)\n",
    "        signal = self.__mix_down(signal)\n",
    "        signal = self.__cut(signal)\n",
    "        signal = self.__right_pad(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        print('Signal : ', len(signal))\n",
    "        return signal , label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = '/home/furkan/AudioDeepLearning/UrbanSound8K/audio/'\n",
    "SAMPLE_RATE = 16000\n",
    "NUM_SAMPLES = 22050\n",
    "\n",
    "transform = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate = SAMPLE_RATE,\n",
    "    n_fft = 1024,\n",
    "    hop_length = 512,\n",
    "    n_mels = 64\n",
    ")\n",
    "\n",
    "\n",
    "dataset = SoundDataset(AUDIO_DIR,transform,SAMPLE_RATE,NUM_SAMPLES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46a0e60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "212cc18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/furkan/AudioDeepLearning/UrbanSound8K/audio/fold5/100032-3-0-0.wav\n",
      "Signal :  2\n",
      "Sample rate :  44100\n",
      "Signal :  1\n"
     ]
    }
   ],
   "source": [
    "signal , label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af78a235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

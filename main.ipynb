{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7d811efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "171519ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDataset(Dataset):\n",
    "    def __init__(self,audio_dir,transformation,target_sample_rate,num_samples,device):\n",
    "        self.df = pd.read_csv('UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def __get_path(self,index):\n",
    "        sample = self.df.iloc[index]\n",
    "        path = os.path.join(self.audio_dir, f\"fold{sample['fold']}\", sample['slice_file_name'])\n",
    "        return path\n",
    "\n",
    "    def __get_label(self,index):\n",
    "        sample = self.df.iloc[index]\n",
    "        return sample['classID']\n",
    "    \n",
    "\n",
    "    def __resample(self,signal,sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            signal = signal.cpu()\n",
    "            resampler = torchaudio.transforms.Resample(sr , self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "\n",
    "        return signal\n",
    "    \n",
    "    def __mix_down(self,signal):\n",
    "        if signal.dim() > 1 and signal.size(0) > 1: # (2,1000) , if it isn't mono\n",
    "            signal = torch.mean(signal, dim = 0 ,keepdim = True)\n",
    "        \n",
    "        return signal\n",
    "\n",
    "    def __cut(self,signal):\n",
    "        if signal.shape[1] > self.num_samples :\n",
    "            signal = signal[:, :self.num_samples]\n",
    "\n",
    "        return signal\n",
    "\n",
    "    def __right_pad(self,signal):\n",
    "        signal_lenght = signal.shape[1]\n",
    "        if signal_lenght < self.num_samples:\n",
    "            missing_samples_num = self.num_samples - signal_lenght\n",
    "            padding = (0,missing_samples_num)\n",
    "            signal = torch.nn.functional.pad(signal,padding)\n",
    "\n",
    "        return signal  \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            audio_sample_path = self.__get_path(index)\n",
    "            label = self.__get_label(index)\n",
    "            signal , sr = torchaudio.load(audio_sample_path, format=\"wav\")\n",
    "            #signal = signal.to(self.device)\n",
    "\n",
    "            signal = self.__resample(signal, sr)\n",
    "            signal = self.__mix_down(signal)\n",
    "            signal = self.__cut(signal)\n",
    "            signal = self.__right_pad(signal)\n",
    "            signal = self.transformation(signal)\n",
    "\n",
    "            return signal, label\n",
    "        except Exception as e:\n",
    "            print(f\"Hata: {e} -> Index: {index}\")\n",
    "            return self.__getitem__((index + 1) % len(self.df))  # bir sonraki veriyle devam et\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6167d276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "AUDIO_DIR = '/home/furkan/AudioDeepLearning/UrbanSound8K/audio/'\n",
    "SAMPLE_RATE = 22050\n",
    "NUM_SAMPLES = 22050\n",
    "LEARNING_RATE = .001\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "\n",
    "transform = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate = SAMPLE_RATE,\n",
    "    n_fft = 1024,\n",
    "    hop_length = 512,\n",
    "    n_mels = 64\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device\",device)\n",
    "\n",
    "\n",
    "dataset = SoundDataset(AUDIO_DIR,transform,SAMPLE_RATE,NUM_SAMPLES,device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "46a0e60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "212cc18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal , label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cb185bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundNeauralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 4 conv , flatten , linear , softmax\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear = nn.Linear(128 * 5 * 4 , 10)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "\n",
    "    def forward(self,input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        predictions = self.softmax(logits)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5a0db594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoundNeauralNetwork(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear): Linear(in_features=2560, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SoundNeauralNetwork().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bf059fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f8726ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(),lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "040f74d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:44<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.4037482738494873\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:40<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.281528949737549\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:40<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.4116017818450928\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:38<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.3438186645507812\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:44<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.289396286010742\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:49<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.326728105545044\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:44<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.173605442047119\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:50<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.3541178703308105\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [05:39<00:00,  4.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.3477509021759033\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:55<00:00,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.355041265487671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    print(f\"Epoch {i+1}\")\n",
    "    for inputs , targets in tqdm(data_loader):\n",
    "        inputs , targets = inputs.to(device) , targets.to(device)\n",
    "\n",
    "        predictions = model(inputs)\n",
    "\n",
    "        loss = loss_f(predictions,targets)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "    print(f\"Loss : {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
